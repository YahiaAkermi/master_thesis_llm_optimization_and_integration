% Introduction
\chapter{Introduction}
Large Language Models (LLMs) have emerged as transformative tools in natural language processing, enabling a wide range of applications across industries. From customer support chatbots to advanced decision-making systems, LLMs are increasingly integrated into enterprise applications to enhance efficiency, scalability, and user experience. However, the integration of LLMs into enterprise systems presents unique challenges, particularly in selecting the most suitable communication protocol for deployment.

Enterprise applications often rely on communication protocols to facilitate interactions between clients and servers. Among these, Representational State Transfer (REST), Model Context Protocol (MCP), and Streaming protocols have gained prominence due to their distinct characteristics and use cases. REST APIs are widely adopted for their simplicity and stateless nature, making them ideal for traditional request-response interactions. MCP, on the other hand, offers message-based communication, enabling asynchronous and context-aware interactions. Streaming protocols, such as WebSocket, provide real-time, bidirectional communication, making them suitable for applications requiring low-latency responses.

The choice of communication protocol significantly impacts the performance, scalability, and user experience of LLM-powered applications. Factors such as latency, throughput, resource utilization, and adaptability to real-time or batch processing scenarios must be carefully considered. Despite the growing adoption of LLMs, there is a lack of comprehensive studies comparing these protocols in the context of enterprise applications.

This thesis aims to address this gap by conducting a comparative study of REST, MCP, and Streaming protocols for integrating LLMs into enterprise systems. By evaluating these protocols across key metrics and use cases, this research seeks to provide actionable insights for enterprise decision-makers. Additionally, the thesis explores the architectural foundations of LLMs, including attention mechanisms and transformer-based models, to establish a technical context for the study.

The findings of this research are expected to contribute to the optimization of LLM integration patterns, enabling enterprises to leverage the full potential of LLMs while addressing the challenges associated with protocol selection. This work not only highlights the trade-offs between protocol simplicity, parallelization capabilities, and real-time responsiveness but also offers a roadmap for future developments in enterprise AI systems.

% Challenges
\chapter{Challenges}
The integration of Large Language Models (LLMs) into enterprise applications introduces several challenges that must be addressed to ensure efficient and reliable deployment. One of the primary challenges is the selection of an appropriate communication protocol. Each protocol—REST, Model Context Protocol (MCP), and Streaming—has its own strengths and limitations, and the choice can significantly impact performance, scalability, and user experience.

REST APIs, while simple and widely adopted, may struggle with high-latency scenarios and lack support for real-time interactions. MCP, with its message-based architecture, offers better support for asynchronous communication but introduces complexity in managing message queues and ensuring context consistency. Streaming protocols, such as WebSocket, excel in real-time, low-latency applications but require robust infrastructure to handle continuous bidirectional communication.

Another challenge lies in optimizing resource utilization. LLMs are computationally intensive, and their integration can strain enterprise infrastructure, leading to increased costs and potential bottlenecks. Balancing resource allocation while maintaining high throughput and low latency is a critical concern. Additionally, ensuring the security and privacy of data exchanged between clients and servers is paramount, especially in industries like finance and healthcare where sensitive information is involved.

% Motivation
\chapter{Motivation}
The rapid adoption of LLMs in enterprise applications underscores the need for optimized integration patterns that can unlock their full potential. Enterprises rely on LLMs for tasks ranging from customer support and content generation to decision-making and predictive analytics. However, the lack of a clear understanding of how different communication protocols impact LLM performance and scalability creates a significant gap in the field.

This thesis is motivated by the need to provide actionable insights into the trade-offs associated with REST, MCP, and Streaming protocols. By systematically evaluating these protocols, this research aims to empower enterprise decision-makers to make informed choices that align with their specific use cases and operational requirements. Furthermore, the study seeks to address the challenges of resource optimization, latency reduction, and real-time responsiveness, which are critical for the successful deployment of LLMs in enterprise environments.

% Organisation and Structure
\chapter{Organisation and Structure}
This thesis is organized into five main parts, each addressing a specific aspect of the research:

\begin{itemize}
    \item \textbf{Part 1: Introduction} \\
    This part introduces the research topic, outlines the challenges and motivation, and presents the objectives and scope of the study.

    \item \textbf{Part 2: Background} \\
    This part provides the theoretical foundation for the study, covering the architecture of LLMs, the principles of REST, MCP, and Streaming protocols, and a review of related work.

    \item \textbf{Part 3: Implementation} \\
    This part details the design and development of a Spring Boot application that integrates a distilled LLM using REST, MCP, and Streaming protocols. It also describes the metrics collection and monitoring setup.

    \item \textbf{Part 4: Results} \\
    This part presents the experimental setup, results, and analysis, highlighting the trade-offs and performance metrics for each protocol.

    \item \textbf{Part 5: Conclusion} \\
    This part summarizes the key findings, discusses the limitations of the study, and suggests directions for future research.
\end{itemize}

The document concludes with a comprehensive bibliography, listing all references cited throughout the thesis.